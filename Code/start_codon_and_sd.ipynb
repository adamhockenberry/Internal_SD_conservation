{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some basic imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "\n",
    "import re\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just some plotting parameters that I'm continually playing around with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams['xtick.labelsize'] = 9\n",
    "matplotlib.rcParams['ytick.labelsize'] = 9\n",
    "matplotlib.rcParams['axes.labelsize'] = 9\n",
    "matplotlib.rcParams['axes.titlesize'] = 10\n",
    "\n",
    "matplotlib.rcParams['axes.grid'] = True\n",
    "matplotlib.rcParams['grid.color'] = '0.8'\n",
    "matplotlib.rcParams['grid.linewidth'] = '0.5'\n",
    "\n",
    "matplotlib.rcParams['axes.edgecolor'] = '0.25'\n",
    "matplotlib.rcParams['xtick.color'] = '0'\n",
    "matplotlib.rcParams['ytick.color'] = '0'\n",
    "\n",
    "matplotlib.rcParams['xtick.major.width'] = 2\n",
    "matplotlib.rcParams['ytick.major.width'] = 2\n",
    "matplotlib.rcParams['ytick.major.size'] = 5\n",
    "matplotlib.rcParams['xtick.major.size'] = 5\n",
    "\n",
    "\n",
    "matplotlib.rcParams['font.family'] = 'sans-serif'\n",
    "matplotlib.rcParams['font.weight']='ultralight'\n",
    "matplotlib.rcParams['axes.axisbelow'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in a genome (genbank file) and getting (separate) dictionaries of coding sequences and 5' UTRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "include = 20 #####This is the length of the UTR that I want extract\n",
    "utr_dicty = {}\n",
    "seq_dicty = {}\n",
    "\n",
    "####You'll want to change this to point to a genome file you have laying around\n",
    "genome = list(SeqIO.parse('../../Genome_database/Data/GBFs/511145.12.PATRIC.gbf', 'genbank'))\n",
    "\n",
    "for record in genome:\n",
    "    for feature in record.features[:]:\n",
    "        if feature.type == 'CDS':\n",
    "            if feature.location.strand == 1:\n",
    "                cds = str(record.seq[feature.location.start:feature.location.end])\n",
    "            elif feature.location.strand == -1:\n",
    "                cds = str(record.seq[feature.location.start:feature.location.end].reverse_complement())\n",
    "            \n",
    "            ###All of these are just filters to screen out weird genes\n",
    "            ###that are too long/short, not multiple of 3, missing stop codons, etc. \n",
    "            if len(cds)%3 != 0 or len(cds) < 180 or len(cds) > 3000 or cds[-3:] not in ['TAA', 'TGA', 'TAG']:\n",
    "                continue\n",
    "            ###Dealing with (by removing) sequences with non-standard bases\n",
    "            if cds.count('A') + cds.count('T') +cds.count('G') +cds.count('C') != len(cds):\n",
    "                continue\n",
    "            ###Also removing any genes with internal stop codons\n",
    "            codons = [str(cds)[i:i+3] for i in range(0, len(cds), 3)]\n",
    "            if codons[:-1].count('TAG') + codons[:-1].count('TGA') + codons[:-1].count('TAA') != 0:\n",
    "                continue\n",
    "            ###Finally, making the dictionaries\n",
    "            name = feature.qualifiers['locus_tag'][0]\n",
    "            if feature.location.strand == 1:\n",
    "                utr_dicty[name] = record.seq[feature.location.start-include:feature.location.start]\n",
    "            elif feature.location.strand == -1:\n",
    "                utr_dicty[name] = record.seq[feature.location.end:feature.location.end+include].reverse_complement()\n",
    "            seq_dicty[name] = cds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nt_seqs = list(seq_dicty.values())\n",
    "print(len(nt_seqs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This reads in a .json file which is a dictionary \n",
    "Where `key::value` pairs are:\n",
    "\n",
    "`nt_fragment::free_energy_of_binding_to_CCUCCU`\n",
    "\n",
    "This dictionary was constructed a little while back from a local install of the ViennaRNA folding software using the cofold method with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../Data/json-energyRef-CCUCCU.txt', 'r') as infile:\n",
    "    fragment_energy_dict = json.load(infile)\n",
    "asd_seq = 'CCUCCU'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This gets the energy landscape so to speak of all the coding sequences and their binding potential to the above loaded putative aSD sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####For my analysis I like to ignore gene ends so as to mitigate the effect\n",
    "####from potentially mis-annotated and/or true start codons (for downstream genes)\n",
    "ignore_last_x = 59 ###To ignore 60, make these 59\n",
    "ignore_first = 59\n",
    "energy_seqs = []\n",
    "for nt_sequence in nt_seqs:\n",
    "    energy_list = []\n",
    "    for i in range(ignore_first, len(nt_sequence)-len(asd_seq)-ignore_last_x):\n",
    "        fragment = nt_sequence[i:i+len(asd_seq)]\n",
    "        energy = fragment_energy_dict[fragment.replace('T', 'U')]\n",
    "        energy_list.append(energy)\n",
    "    energy_seqs.append(energy_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the part of the code used in the paper where I look for ATG codons in various downstream windows from strong aSD binding sites inside of the genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "windows_to_search = []\n",
    "for i in range(0, 16, 2):\n",
    "    windows_to_search.append((i,i+6))\n",
    "\n",
    "thresh = -7.5 ###Where threshold is how strong the sequence must bind the aSD to be considered\n",
    "\n",
    "results_dicty_actual = {}\n",
    "results_dicty_control = {}\n",
    "\n",
    "randomizations = 5 ###Randomizations here aren't exactly fast so keeping this number small will help\n",
    "\n",
    "for window_to_search in windows_to_search:\n",
    "    putative_starts = []\n",
    "    sd_counts_per_gene = []\n",
    "    ###Essentially iterate through each gene\n",
    "    for i, energy_list in enumerate(energy_seqs):\n",
    "        sd_counts = 0\n",
    "        ###Find the positions that bind strongly to the putative aSD sequence\n",
    "        for pos,energy in enumerate(energy_list):\n",
    "            if pos == 0:\n",
    "                continue\n",
    "            if pos == len(energy_list)-1:\n",
    "                continue\n",
    "            ###This little bit makes sure that the position I found is the strongest of its flanking sequences\n",
    "            ###and is the reason for the if: continue statements above\n",
    "            ###Essentially it prevents one strong SD-like sequence from showing up multiple times by making\n",
    "            ###sure it's stronger than immediate neighbors.\n",
    "            if energy < thresh and energy <= energy_list[pos-1] and energy < energy_list[pos+1]:\n",
    "                sd_counts += 1\n",
    "                ###Now that I found a SD-like sequence, grab the downstream nt fragment\n",
    "                fragment = nt_seqs[i][pos+ignore_first+len(asd_seq)+window_to_search[0]:\\\n",
    "                                      pos+ignore_first+len(asd_seq)+window_to_search[1]]\n",
    "                assert len(fragment)==6\n",
    "                ###Look for start codons\n",
    "                if 'ATG' in fragment:\n",
    "                    position = fragment.index('ATG')\n",
    "                    absolute_position = pos+ignore_first+len(asd_seq)+window_to_search[0]+position\n",
    "                    putative_starts.append(absolute_position)\n",
    "        sd_counts_per_gene.append(sd_counts)\n",
    "    print(window_to_search, np.sum(sd_counts_per_gene))\n",
    "    \n",
    "    ####Pretty much repeat the process with some shuffling\n",
    "    control_starts_matrix = []\n",
    "    for randomization in range(randomizations):\n",
    "        control_starts = []\n",
    "        for i, sd_counts in enumerate(sd_counts_per_gene):\n",
    "            if sd_counts > 0:\n",
    "                ###Here is where i randomly choose a position to extract the downstream fragment from\n",
    "                positions_to_test = np.random.choice(list(range(len(energy_seqs[i]))), replace=False, size=sd_counts)\n",
    "                for pos in positions_to_test:\n",
    "                    fragment = nt_seqs[i][pos+ignore_first+len(asd_seq)+window_to_search[0]:\\\n",
    "                                          pos+ignore_first+len(asd_seq)+window_to_search[1]]\n",
    "                    if 'ATG' in fragment:\n",
    "                        position = fragment.index('ATG')\n",
    "                        absolute_position = pos+ignore_first+len(asd_seq)+window_to_search[0]+position\n",
    "                        control_starts.append(absolute_position)\n",
    "        control_starts_matrix.append(control_starts)\n",
    "    \n",
    "\n",
    "    results_dicty_actual[window_to_search] = putative_starts\n",
    "    results_dicty_control[window_to_search] = control_starts_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# And plotting those results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = len(windows_to_search)\n",
    "actual_vals = [len(results_dicty_actual[window]) for window in windows_to_search]\n",
    "control_vals_mean = []\n",
    "control_vals_stdev = []\n",
    "for window in windows_to_search:\n",
    "    tempy = []\n",
    "    for i in range(randomizations):\n",
    "        tempy.append(len(results_dicty_control[window][i]))\n",
    "    control_vals_mean.append(np.mean(tempy))\n",
    "    control_vals_stdev.append(np.std(tempy))\n",
    "    \n",
    "for i in range(len(actual_vals)):\n",
    "    zscore = (actual_vals[i]-control_vals_mean[i])/control_vals_stdev[i]\n",
    "    print(windows_to_search[i], zscore, stats.norm.sf(abs(zscore))*2, actual_vals[i]/control_vals_mean[i])\n",
    "\n",
    "\n",
    "plot_indices = np.arange(n)\n",
    "width = 0.4\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4.5,3))\n",
    "rects1 = ax.bar(plot_indices, actual_vals, width, color='#2ca02c') \n",
    "rects2 = ax.bar(plot_indices+width, control_vals_mean, width, yerr=control_vals_stdev, color='thistle') \n",
    "\n",
    "ax.set_xticks(plot_indices + width/2)\n",
    "ax.set_xticklabels(windows_to_search, rotation=45)\n",
    "ax.set_ylabel('ATG counts')\n",
    "ax.set_xlabel('3\\' downstream window')\n",
    "legend = ax.legend((rects1[0], rects2[0]), ('Actual SD-like sites', 'Random sites'),\\\n",
    "                   fontsize=10, loc=3, bbox_to_anchor=(0.01, 1.02, 1., .102), ncol=2, borderaxespad=0.)\n",
    "legend.get_frame().set_alpha(1)\n",
    "# ax.set_ylim(0, 2000)\n",
    "plt.tight_layout()\n",
    "# plt.savefig('../Results/Figures/start_codon_frequency.pdf', bbox_inches='tight')\n",
    "# plt.savefig('/Users/adamhockenberry/Desktop/Caulo_start_codon_and_SD_thresh=-8.5.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Null expectation about total number of SD genes and SD-like counts throughout the *E. coli* genome\n",
    "\n",
    "So now we don't care about the whole start codon analysis. This bit of code needn't be in this notebook it's just to find SD-like sequences and compare those numbers to null. Code was borowed from a past paper with some changes (Yang, Hockenberry, et al. G3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from Bio.Data import CodonTable\n",
    "\n",
    "def get_codon_dicts(n=11):\n",
    "    '''\n",
    "    This function outputs two dictionaries to convert between amino acids and codons and vice versa.\n",
    "    No necessary imports unless you want to use a strange genetic code table.\n",
    "    \n",
    "    I've subsequently found much cleaner ways to do this, but alas here it is for the paper.\n",
    "    '''\n",
    "    GivenCodonTable = CodonTable.unambiguous_dna_by_id[n]\n",
    "    nucleotides = ['A', 'T', 'G', 'C']\n",
    "    aa_to_codons = {}\n",
    "    for first in nucleotides:\n",
    "        for second in nucleotides:\n",
    "            for third in nucleotides:\n",
    "                Codon = first + second + third\n",
    "                if Codon not in CodonTable.unambiguous_dna_by_id[n].stop_codons:\n",
    "                    if GivenCodonTable.forward_table[Codon] in aa_to_codons.keys():\n",
    "                            aa_to_codons[GivenCodonTable.forward_table[Codon]].append(Codon)\n",
    "                    else:\n",
    "                        aa_to_codons[GivenCodonTable.forward_table[Codon]] = [Codon]\n",
    "                else:\n",
    "                    pass\n",
    "    codon_to_aa = {}\n",
    "    for aa, syns in aa_to_codons.items():\n",
    "        for syn in syns:\n",
    "            codon_to_aa[syn] = aa\n",
    "    return aa_to_codons, codon_to_aa\n",
    "\n",
    "def shuffle_gene_synonymous(gene_sequence_codons, aa_to_codons_dict, codon_to_aa_dict):\n",
    "    '''\n",
    "    This is my randomization procedure that I think is pretty efficient.\n",
    "    What it does is essentially break every 'gene_sequence_codons' list down into a dictionary (\"temp_dict\")\n",
    "    that has key:val of \"amino acid\":\"list of all codons used in sequence for that amino acid\". Then shuffles\n",
    "    those resulting lists and re-codes the original gene_sequence_codons\n",
    "    '''\n",
    "    shuffled_sequence_codons = []\n",
    "    temp_dict = {}\n",
    "    for aa in aa_to_codons_dict:\n",
    "        temp_dict[aa] = []\n",
    "    for codon in gene_sequence_codons:\n",
    "        temp_dict[codon_to_aa_dict[codon]].append(codon)\n",
    "    for aa, synon_codons in temp_dict.items():\n",
    "        np.random.shuffle(synon_codons)\n",
    "    for codon in gene_sequence_codons:\n",
    "        shuffled_sequence_codons.append(temp_dict[codon_to_aa_dict[codon]].pop())\n",
    "    return shuffled_sequence_codons\n",
    "\n",
    "aa_to_codons_dict, codon_to_aa_dict = get_codon_dicts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First get coding sequence and 5' UTR dictionaries\n",
    "\n",
    "...repeating what I did at the top of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "include = 20\n",
    "utr_dicty = {}\n",
    "seq_dicty = {}\n",
    "\n",
    "\n",
    "# genome = list(SeqIO.parse('/Users/adamhockenberry/Projects/2017/Genome_database/Data/GBFs/511145.12.PATRIC.gbf', 'genbank'))\n",
    "for record in genome:\n",
    "    for feature in record.features[:]:\n",
    "        if feature.type == 'CDS':\n",
    "            if feature.location.strand == 1:\n",
    "                cds = str(record.seq[feature.location.start:feature.location.end])\n",
    "            elif feature.location.strand == -1:\n",
    "                cds = str(record.seq[feature.location.start:feature.location.end].reverse_complement())\n",
    "            if len(cds)%3 != 0 or len(cds) < 180 or len(cds) > 3000 or cds[-3:] not in ['TAA', 'TGA', 'TAG']:\n",
    "                continue\n",
    "            if cds.count('A') + cds.count('T') +cds.count('G') +cds.count('C') != len(cds):\n",
    "                continue\n",
    "            codons = [str(cds)[i:i+3] for i in range(0, len(cds), 3)]\n",
    "            if codons[:-1].count('TAG') + codons[:-1].count('TGA') + codons[:-1].count('TAA') != 0:\n",
    "                continue\n",
    "            name = feature.qualifiers['locus_tag'][0]\n",
    "            if feature.location.strand == 1:\n",
    "                utr_dicty[name] = record.seq[feature.location.start-include:feature.location.start]\n",
    "            elif feature.location.strand == -1:\n",
    "                utr_dicty[name] = record.seq[feature.location.end:feature.location.end+include].reverse_complement()\n",
    "            seq_dicty[name] = cds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(seq_dicty.keys()), len(utr_dicty.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now assess SD sequence usage in the UTRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../Data/json-energyRef-CCUCCU.txt') as infile:\n",
    "    fragment_energy_dict = json.load(infile)\n",
    "asd_seq = 'CCTCCT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# randomizations = 100\n",
    "randomizations = 2\n",
    "real_utrs = [str(j) for i,j in utr_dicty.items()]\n",
    "\n",
    "strongest_real = []\n",
    "strongest_fake_all = []\n",
    "strongest_fake_dict = {}\n",
    "for randomization in range(randomizations):\n",
    "    strongest_fake_dict[randomization] = []\n",
    "\n",
    "\n",
    "for utr in real_utrs[:]:\n",
    "    energy_list = []\n",
    "    for i in range(0, len(utr)-len(asd_seq)-4):\n",
    "        fragment = utr[i:i+len(asd_seq)]\n",
    "        energy = fragment_energy_dict[fragment.replace('T', 'U')]\n",
    "        energy_list.append(energy)\n",
    "    strongest_real.append(min(energy_list))\n",
    "    \n",
    "    for randomization in range(randomizations):\n",
    "        energy_list = []\n",
    "        utr_rand = np.array(list(utr))\n",
    "        np.random.shuffle(utr_rand)\n",
    "        utr_rand = ''.join(list(utr_rand))\n",
    "        for i in range(0, len(utr_rand)-len(asd_seq)-4):\n",
    "            fragment = utr_rand[i:i+len(asd_seq)]\n",
    "            energy = fragment_energy_dict[fragment.replace('T', 'U')]\n",
    "            energy_list.append(energy)\n",
    "        strongest_fake_all.append(min(energy_list))\n",
    "        strongest_fake_dict[randomization].append(min(energy_list))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(3,2))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.hist(strongest_fake_all, 60, normed=True, alpha=0.7, color='gray', label='Shuffled UTRs')\n",
    "ax.hist(strongest_real, 60, normed=True, alpha=0.7, label='Real UTRs')\n",
    "ax.axvline(-4.5, c='r')\n",
    "ax.axvline(-3.5, c='r', linestyle='--')\n",
    "ax.axvline(-5.5, c='r', linestyle=':')\n",
    "ax.set_xlabel('UTR binding strength to \"5-CCUCCU-3\"')\n",
    "ax.set_ylabel('Probability density')\n",
    "fig.subplots_adjust(top=0.8, left=0.2, right=0.9, bottom=0.25)  # create some space below the plots by increasing the bottom-value\n",
    "ax.legend(loc='upper center', ncol=2, bbox_to_anchor=(0.15, 0.9, 0.7, 0.4), fontsize=8)\n",
    "# plt.savefig('../Results/Figures/SD_threshold.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "threshold = -8.5\n",
    "\n",
    "observed = len([i for i in strongest_real if i < threshold])\n",
    "expectation = [len([i for i in strongest_fake_dict[randomization] if i < threshold]) for randomization in strongest_fake_dict.keys()]\n",
    "\n",
    "\n",
    "zscore = (observed-np.mean(expectation))/np.std(expectation)\n",
    "print(len(strongest_real), observed, np.mean(expectation), zscore, stats.norm.sf(abs(zscore))*2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now count the number of SD-like sequences subject to restrictions\n",
    "\n",
    "**Restrictions to note are that I don't consider the first 60 and last 60 nucleotides of each gene and (this is critical) I try to avoid double counting strong SD-like sequences by only counting sequences that are below the binding energy threshold and which are more negative than their neighbors directly to the left and right**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nt_seqs = list(seq_dicty.values())\n",
    "print(len(nt_seqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ignore_last_x = 59\n",
    "ignore_first = 59\n",
    "energy_seqs = []\n",
    "for nt_sequence in nt_seqs:\n",
    "    energy_list = []\n",
    "    for i in range(ignore_first, len(nt_sequence)-len(asd_seq)-ignore_last_x):\n",
    "        fragment = nt_sequence[i:i+len(asd_seq)]\n",
    "        energy = fragment_energy_dict[fragment.replace('T', 'U')]\n",
    "        energy_list.append(energy)\n",
    "    energy_seqs.append(energy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###Important parameter here! What binding strength do you want to consider a fragment as\n",
    "###being an SD-like sequence?\n",
    "threshold = -3.5\n",
    "observed_counts = 0\n",
    "for energy_gene in energy_seqs:\n",
    "    for i,energy in enumerate(energy_gene):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        if i == len(energy_gene)-1:\n",
    "            continue\n",
    "        ###This is where I can toggle to either consider all windows or only\n",
    "        ###windows where the immediate neighbors are higher (i.e. avoid double counting)\n",
    "#             if energy <= threshold: ##Consider each independent window\n",
    "        if energy < threshold and energy <= energy_gene[i-1] and energy < energy_gene[i+1]:\n",
    "            observed_counts += 1\n",
    "\n",
    "print(observed_counts)\n",
    "print(len(energy_seqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rand_counts = []\n",
    "for randomization in range(100):\n",
    "    rand_energy_seqs = []\n",
    "    for nt_sequence_initial in nt_seqs:\n",
    "        codon_list = [nt_sequence_initial[nt:nt+3] for nt in range(0, len(nt_sequence_initial), 3)]\n",
    "        stop = codon_list[-1]\n",
    "        codon_list = codon_list[:-1]\n",
    "#         ###Expectation if you shuffled all the codons\n",
    "#         np.random.shuffle(codon_list)\n",
    "#         nt_sequence = ''.join(codon_list)\n",
    "#         ###Expectation if you shuffle while preserving primary amino acid sequence\n",
    "        codon_list_rand = shuffle_gene_synonymous(codon_list, aa_to_codons_dict, codon_to_aa_dict)\n",
    "        codon_list_rand.append(stop)\n",
    "        nt_sequence = ''.join(codon_list_rand)\n",
    "        energy_list = []\n",
    "        for i in range(ignore_first, len(nt_sequence)-len(asd_seq)-ignore_last_x):\n",
    "            fragment = nt_sequence[i:i+len(asd_seq)]\n",
    "            energy = fragment_energy_dict[fragment.replace('T', 'U')]\n",
    "            energy_list.append(energy)\n",
    "        rand_energy_seqs.append(energy_list)\n",
    "    \n",
    "    counts = 0\n",
    "    for energy_gene in rand_energy_seqs:\n",
    "        for i,energy in enumerate(energy_gene):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            if i == len(energy_gene)-1:\n",
    "                continue\n",
    "            if energy < threshold and energy <= energy_gene[i-1] and energy < energy_gene[i+1]:\n",
    "                counts += 1\n",
    "\n",
    "    print('#####Randomization number: {} found {} counts'.format(randomization, counts))\n",
    "    rand_counts.append(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zscore = (observed_counts-np.mean(rand_counts))/np.std(rand_counts)\n",
    "print(observed_counts, np.mean(rand_counts), zscore, stats.norm.sf(abs(zscore))*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
